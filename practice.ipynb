{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b0ffe6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acc23f",
   "metadata": {},
   "source": [
    "1. Построить сеть, которая состоит из последовательности двух слоев Dense, которые являются полносвязными нейронными слоями. Второй  слой — это 3-переменный слой потерь, возвращающий массив с 3 оценками вероятностей (в сумме дающих 1). Каждая оценка определяет вероятность принадлежности текущего изображения к одному из 3 классов. Обучения происходит со следующими параметрами fit: количество эпох – 10, размер входных данных – 10, размер тестовых данных – 30% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d18422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.datasets.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f001d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2031a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images/255.0\n",
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5683e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(labels, idxs):\n",
    "    label_idxs, y = [], []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in idxs:\n",
    "            label_idxs.append(i)\n",
    "            y.append(labels[i])\n",
    "    return label_idxs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "21223899",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_train_labels, y_train = get_index(train_labels, idxs=[0, 1, 2])\n",
    "correct_test_labels, y_test = get_index(test_labels, idxs=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20a0f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f30310d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(images, correct_labels):\n",
    "    x = []\n",
    "    for i in range(len(images)):\n",
    "        if i in correct_labels:\n",
    "            x.append(images[i].reshape(-1).tolist())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "477db477",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_values(train_images, correct_train_labels)\n",
    "X_test = get_values(test_images, correct_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "30288081",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9983e583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0605 - accuracy: 0.9815 - val_loss: 0.0224 - val_accuracy: 0.9930\n",
      "Epoch 2/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0259 - accuracy: 0.9922 - val_loss: 0.0209 - val_accuracy: 0.9930\n",
      "Epoch 3/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0162 - accuracy: 0.9959 - val_loss: 0.0206 - val_accuracy: 0.9924\n",
      "Epoch 4/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0208 - val_accuracy: 0.9936\n",
      "Epoch 5/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 0.0169 - val_accuracy: 0.9943\n",
      "Epoch 6/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0153 - val_accuracy: 0.9943\n",
      "Epoch 7/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.0180 - val_accuracy: 0.9949\n",
      "Epoch 8/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
      "Epoch 9/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0156 - val_accuracy: 0.9946\n",
      "Epoch 10/10\n",
      "745/745 [==============================] - 2s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0159 - val_accuracy: 0.9952\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "    \n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "task1 = model.fit(X_train, y_train, verbose=1, epochs=10, batch_size=25, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bcbc2",
   "metadata": {},
   "source": [
    "2. Найти или создать любой датасет под задачу бинарной классификации и провести обучение модели с объяснением выбора функции потерь, оптимизатора и метрик для мониторинга на этапах обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1b34559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   R  \n",
       "1    0.0052  0.0044   R  \n",
       "2    0.0095  0.0078   R  \n",
       "3    0.0040  0.0117   R  \n",
       "4    0.0107  0.0094   R  \n",
       "..      ...     ...  ..  \n",
       "203  0.0193  0.0157   M  \n",
       "204  0.0062  0.0067   M  \n",
       "205  0.0077  0.0031   M  \n",
       "206  0.0036  0.0048   M  \n",
       "207  0.0061  0.0115   M  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('sonar.csv', header=None)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ceb8cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0bca1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "739224ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "13/13 [==============================] - 1s 13ms/step - loss: 0.6932 - accuracy: 0.5027 - val_loss: 0.6950 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6922 - accuracy: 0.5348 - val_loss: 0.6973 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6910 - accuracy: 0.6257 - val_loss: 0.6952 - val_accuracy: 0.2381\n",
      "Epoch 4/40\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.7754 - val_loss: 0.6985 - val_accuracy: 0.1905\n",
      "Epoch 5/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.7540 - val_loss: 0.6993 - val_accuracy: 0.2857\n",
      "Epoch 6/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6758 - accuracy: 0.7594 - val_loss: 0.7135 - val_accuracy: 0.1905\n",
      "Epoch 7/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.7380 - val_loss: 0.7257 - val_accuracy: 0.1905\n",
      "Epoch 8/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6460 - accuracy: 0.7380 - val_loss: 0.7120 - val_accuracy: 0.3333\n",
      "Epoch 9/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6270 - accuracy: 0.7540 - val_loss: 0.7055 - val_accuracy: 0.3810\n",
      "Epoch 10/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6023 - accuracy: 0.7594 - val_loss: 0.7581 - val_accuracy: 0.2857\n",
      "Epoch 11/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5777 - accuracy: 0.7594 - val_loss: 0.7846 - val_accuracy: 0.2857\n",
      "Epoch 12/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5518 - accuracy: 0.7487 - val_loss: 0.7761 - val_accuracy: 0.3333\n",
      "Epoch 13/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5335 - accuracy: 0.7701 - val_loss: 0.7272 - val_accuracy: 0.3810\n",
      "Epoch 14/40\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5216 - accuracy: 0.7647 - val_loss: 0.7592 - val_accuracy: 0.3810\n",
      "Epoch 15/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5079 - accuracy: 0.7326 - val_loss: 0.7364 - val_accuracy: 0.3810\n",
      "Epoch 16/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4908 - accuracy: 0.7861 - val_loss: 0.7215 - val_accuracy: 0.4286\n",
      "Epoch 17/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7594 - val_loss: 0.6855 - val_accuracy: 0.4286\n",
      "Epoch 18/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4713 - accuracy: 0.7861 - val_loss: 0.7279 - val_accuracy: 0.4286\n",
      "Epoch 19/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.7861 - val_loss: 0.6398 - val_accuracy: 0.5238\n",
      "Epoch 20/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4513 - accuracy: 0.7647 - val_loss: 0.6917 - val_accuracy: 0.4762\n",
      "Epoch 21/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4448 - accuracy: 0.8182 - val_loss: 0.5959 - val_accuracy: 0.6190\n",
      "Epoch 22/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4354 - accuracy: 0.7914 - val_loss: 0.6455 - val_accuracy: 0.5238\n",
      "Epoch 23/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4297 - accuracy: 0.8128 - val_loss: 0.5676 - val_accuracy: 0.6667\n",
      "Epoch 24/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4265 - accuracy: 0.8128 - val_loss: 0.6074 - val_accuracy: 0.5714\n",
      "Epoch 25/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4213 - accuracy: 0.8075 - val_loss: 0.5575 - val_accuracy: 0.6667\n",
      "Epoch 26/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4175 - accuracy: 0.8075 - val_loss: 0.5536 - val_accuracy: 0.6667\n",
      "Epoch 27/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4085 - accuracy: 0.8182 - val_loss: 0.4971 - val_accuracy: 0.7619\n",
      "Epoch 28/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4061 - accuracy: 0.8342 - val_loss: 0.5884 - val_accuracy: 0.5714\n",
      "Epoch 29/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4125 - accuracy: 0.7861 - val_loss: 0.4033 - val_accuracy: 0.8571\n",
      "Epoch 30/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4065 - accuracy: 0.7968 - val_loss: 0.4855 - val_accuracy: 0.7619\n",
      "Epoch 31/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3956 - accuracy: 0.8182 - val_loss: 0.5010 - val_accuracy: 0.7619\n",
      "Epoch 32/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3908 - accuracy: 0.8235 - val_loss: 0.5288 - val_accuracy: 0.7143\n",
      "Epoch 33/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3859 - accuracy: 0.8182 - val_loss: 0.4635 - val_accuracy: 0.7619\n",
      "Epoch 34/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3811 - accuracy: 0.8289 - val_loss: 0.4526 - val_accuracy: 0.7619\n",
      "Epoch 35/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3803 - accuracy: 0.8289 - val_loss: 0.4719 - val_accuracy: 0.7619\n",
      "Epoch 36/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3764 - accuracy: 0.8342 - val_loss: 0.4574 - val_accuracy: 0.7619\n",
      "Epoch 37/40\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8235 - val_loss: 0.3847 - val_accuracy: 0.8571\n",
      "Epoch 38/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3784 - accuracy: 0.8289 - val_loss: 0.4763 - val_accuracy: 0.7619\n",
      "Epoch 39/40\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3679 - accuracy: 0.8235 - val_loss: 0.4096 - val_accuracy: 0.7619\n",
      "Epoch 40/40\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8396 - val_loss: 0.3946 - val_accuracy: 0.8095\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='Adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "task2 = model.fit(X, encoded_Y, batch_size=15, epochs=40, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b4f44",
   "metadata": {},
   "source": [
    "3. Найти или создать любой датасет под задачу проведения перекрестной проверки по K блокам. Вывести параметр среднеквадратичной ошибки при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3f7e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='Adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c43132b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(k:int, num_val_samples:int, num_epochs:list[int])->list:\n",
    "    all_scores = []\n",
    "    infl = []  \n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f'\\nProcessing fold №{i}')\n",
    "\n",
    "        val_data = X[i*num_val_samples: (i+1)*num_val_samples]\n",
    "        val_targets = encoded_Y[i*num_val_samples: (i+1)*num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate([X[:i*num_val_samples], \n",
    "                                                 X[(i+1)*num_val_samples:]], axis=0)\n",
    "        partial_train_targets = np.concatenate([encoded_Y[:i*num_val_samples], \n",
    "                                                    encoded_Y[(i+1)*num_val_samples:]], axis=0)\n",
    "        model = build_model()\n",
    "        task3 = model.fit(partial_train_data, partial_train_targets, \n",
    "                                    epochs=50, batch_size=20, verbose=1)\n",
    "        val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
    "            \n",
    "    return infl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "91d03878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing fold №0\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2487 - mae: 0.4987\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2451 - mae: 0.4950\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2397 - mae: 0.4892\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2310 - mae: 0.4789\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2185 - mae: 0.4615\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2057 - mae: 0.4367\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1987 - mae: 0.4093\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1965 - mae: 0.3910\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1962 - mae: 0.3882\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1949 - mae: 0.3871\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1939 - mae: 0.3909\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1917 - mae: 0.3926\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1901 - mae: 0.3901\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1886 - mae: 0.3870\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1865 - mae: 0.3865\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1851 - mae: 0.3880\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1822 - mae: 0.3828\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1796 - mae: 0.3783\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 747us/step - loss: 0.1763 - mae: 0.3747\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1732 - mae: 0.3728\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1690 - mae: 0.3680\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1646 - mae: 0.3627\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1604 - mae: 0.3574\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3540\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1514 - mae: 0.3408\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1468 - mae: 0.3341\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1428 - mae: 0.3268\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1387 - mae: 0.3172\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1341 - mae: 0.3136\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1298 - mae: 0.3093\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1264 - mae: 0.3062\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1213 - mae: 0.2912\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1171 - mae: 0.2862\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1123 - mae: 0.2776\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1081 - mae: 0.2793\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1036 - mae: 0.2666\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1005 - mae: 0.2651\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0929 - mae: 0.2498\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0899 - mae: 0.2373\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0846 - mae: 0.2333\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.0818 - mae: 0.2335\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0786 - mae: 0.2186\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0769 - mae: 0.2245\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0717 - mae: 0.2054\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0671 - mae: 0.1968\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 157us/step - loss: 0.0662 - mae: 0.2016\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0625 - mae: 0.1899\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0596 - mae: 0.1869\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0576 - mae: 0.1783\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0554 - mae: 0.1722\n",
      "WARNING:tensorflow:6 out of the last 89 calls to <function Model.make_test_function.<locals>.test_function at 0x0000022477458550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.4274 - mae: 0.5603\n",
      "\n",
      "Processing fold №1\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 2ms/step - loss: 0.2482 - mae: 0.4982\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2446 - mae: 0.4943\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2398 - mae: 0.4889\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2344 - mae: 0.4819\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2270 - mae: 0.4708\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2201 - mae: 0.4575\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2154 - mae: 0.4432\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2119 - mae: 0.4339\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2097 - mae: 0.4268\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2069 - mae: 0.4239\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2033 - mae: 0.4200\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2025 - mae: 0.4240\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1958 - mae: 0.4169\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1925 - mae: 0.4077\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1872 - mae: 0.4034\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1839 - mae: 0.4040\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1784 - mae: 0.3920\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1735 - mae: 0.3824\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1690 - mae: 0.3731\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1640 - mae: 0.3667\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1622 - mae: 0.3698\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.1571 - mae: 0.3532\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3417\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1494 - mae: 0.3406\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.1486 - mae: 0.3428\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1459 - mae: 0.3292\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1431 - mae: 0.3211\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1425 - mae: 0.3278\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1387 - mae: 0.3152\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1369 - mae: 0.3078\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1346 - mae: 0.3096\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.3047\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 888us/step - loss: 0.1324 - mae: 0.2974\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1308 - mae: 0.2946\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1291 - mae: 0.2930\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1277 - mae: 0.2869\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1271 - mae: 0.2897\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1254 - mae: 0.2856\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1249 - mae: 0.2824\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1261 - mae: 0.2781\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.1227 - mae: 0.2803\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1233 - mae: 0.2775\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1207 - mae: 0.2726\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1204 - mae: 0.2704\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1195 - mae: 0.2733\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1172 - mae: 0.2688\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1173 - mae: 0.2612\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1176 - mae: 0.2638\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1158 - mae: 0.2595\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1147 - mae: 0.2612\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.2860 - mae: 0.4808\n",
      "\n",
      "Processing fold №2\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 2ms/step - loss: 0.2497 - mae: 0.4997\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2488 - mae: 0.4988\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2475 - mae: 0.4974\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2455 - mae: 0.4952\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2427 - mae: 0.4918\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2385 - mae: 0.4860\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2349 - mae: 0.4795\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2308 - mae: 0.4717\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2279 - mae: 0.4652\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2249 - mae: 0.4589\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2206 - mae: 0.4536\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2155 - mae: 0.4500\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2096 - mae: 0.4434\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2045 - mae: 0.4393\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1951 - mae: 0.4294\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1890 - mae: 0.4150\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1792 - mae: 0.4016\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1716 - mae: 0.3951\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1643 - mae: 0.3817\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1589 - mae: 0.3649\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1504 - mae: 0.3539\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1463 - mae: 0.3462\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1409 - mae: 0.3310\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1349 - mae: 0.3227\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1317 - mae: 0.3184\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1273 - mae: 0.3023\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1235 - mae: 0.2943\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 598us/step - loss: 0.1210 - mae: 0.2953\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1176 - mae: 0.2843\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1152 - mae: 0.2739\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1127 - mae: 0.2719\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1123 - mae: 0.2676\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1084 - mae: 0.2573\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1077 - mae: 0.2587\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1065 - mae: 0.2571\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1044 - mae: 0.2469\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1035 - mae: 0.2435\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1017 - mae: 0.2433\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0994 - mae: 0.2374\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0982 - mae: 0.2343\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0971 - mae: 0.2331\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0956 - mae: 0.2291\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0945 - mae: 0.2262\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0949 - mae: 0.2230\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0928 - mae: 0.2237\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0911 - mae: 0.2211\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0900 - mae: 0.2150\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0894 - mae: 0.2135\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0885 - mae: 0.2146\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0868 - mae: 0.2081\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5945 - mae: 0.7112\n",
      "\n",
      "Processing fold №3\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 2ms/step - loss: 0.2492 - mae: 0.4992\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2475 - mae: 0.4974\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2452 - mae: 0.4948\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2422 - mae: 0.4910\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2378 - mae: 0.4849\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2334 - mae: 0.4772\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2277 - mae: 0.4678\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2224 - mae: 0.4585\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2154 - mae: 0.4476\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2084 - mae: 0.4419\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1994 - mae: 0.4319\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1911 - mae: 0.4161\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1829 - mae: 0.4041\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1757 - mae: 0.3930\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.1714 - mae: 0.3762\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1631 - mae: 0.3675\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1597 - mae: 0.3677\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3459\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1522 - mae: 0.3446\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1440 - mae: 0.3308\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1423 - mae: 0.3277\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1381 - mae: 0.3152\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1314 - mae: 0.3055\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1296 - mae: 0.3047\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1256 - mae: 0.2952\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1236 - mae: 0.2891\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1222 - mae: 0.2831\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1246 - mae: 0.2863\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.1159 - mae: 0.2701\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1164 - mae: 0.2694\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1127 - mae: 0.2663\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 0s/step - loss: 0.1105 - mae: 0.2613\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1118 - mae: 0.2570\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1064 - mae: 0.2523\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1063 - mae: 0.2518\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1036 - mae: 0.2434\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1032 - mae: 0.2433\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1021 - mae: 0.2420\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1005 - mae: 0.2371\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0984 - mae: 0.2332\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0983 - mae: 0.2301\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0949 - mae: 0.2270\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0935 - mae: 0.2246\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0943 - mae: 0.2219\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0942 - mae: 0.2236\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0924 - mae: 0.2164\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0912 - mae: 0.2163\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0876 - mae: 0.2114\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0867 - mae: 0.2088\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0855 - mae: 0.2058\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5139 - mae: 0.6347\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "num_val_samples = len(X)//k\n",
    "task_3 = learning(k, num_val_samples, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbd7e3",
   "metadata": {},
   "source": [
    "4. Аналогично 4 лабораторной работы, провести подготовку к обучению модели по распознаванию рукописного текста (букв) на основе библиотеки Extended MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d604c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emnist\n",
      "  Downloading emnist-0.0-py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from emnist) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from emnist) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from emnist) (4.64.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->emnist) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->emnist) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->emnist) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->emnist) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->emnist) (0.4.6)\n",
      "Installing collected packages: emnist\n",
      "Successfully installed emnist-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bd832ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import list_datasets, extract_training_samples\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dbb83d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading emnist.zip: 536MB [07:08, 1.31MB/s]                                                                        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['balanced', 'byclass', 'bymerge', 'digits', 'letters', 'mnist']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5a1d9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124800, 28, 28)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = extract_training_samples('letters')\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a39f3271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0], cmap = 'binary')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "eaa87cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image(img):\n",
    "    plt.imshow(img, cmap = 'binary')\n",
    "    plt.show()\n",
    "    \n",
    "    size = (28, 28)\n",
    "    img = np.array(img, dtype=np.uint8).reshape(-1, np.prod(size)) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1f129e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.01568627, 0.01568627,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00784314, 0.01568627, 0.01960784,\n",
       "        0.03529412, 0.1254902 , 0.1254902 , 0.08235294, 0.08235294,\n",
       "        0.07843137, 0.03529412, 0.01568627, 0.00784314, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.08235294, 0.13333333,\n",
       "        0.32156863, 0.49019608, 0.50588235, 0.54901961, 0.79607843,\n",
       "        0.79607843, 0.68235294, 0.6745098 , 0.66666667, 0.54901961,\n",
       "        0.49019608, 0.24705882, 0.07058824, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00392157, 0.07843137, 0.13333333,\n",
       "        0.32941176, 0.66666667, 0.8       , 0.91372549, 0.97647059,\n",
       "        0.98039216, 0.98039216, 0.99607843, 0.99607843, 0.98823529,\n",
       "        0.98823529, 0.98823529, 0.98039216, 0.92941176, 0.43529412,\n",
       "        0.1254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.03137255, 0.30196078, 0.45098039, 0.6745098 , 0.90980392,\n",
       "        0.96078431, 0.98823529, 0.99607843, 0.99607843, 0.99607843,\n",
       "        0.99607843, 0.99607843, 0.99607843, 0.99607843, 0.99215686,\n",
       "        0.96862745, 0.80784314, 0.29803922, 0.07843137, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02745098, 0.35294118, 0.85490196,\n",
       "        0.95294118, 0.98823529, 0.99607843, 0.99607843, 0.99607843,\n",
       "        0.99607843, 0.99607843, 1.        , 1.        , 0.99607843,\n",
       "        0.99607843, 0.96862745, 0.81176471, 0.62352941, 0.18431373,\n",
       "        0.02745098, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "        0.08627451, 0.62352941, 0.96078431, 0.99215686, 0.99607843,\n",
       "        0.99607843, 0.98823529, 0.98431373, 0.99607843, 0.99607843,\n",
       "        0.99607843, 0.99607843, 0.99607843, 0.99215686, 0.86666667,\n",
       "        0.49803922, 0.30980392, 0.03921569, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00392157, 0.14117647, 0.37254902, 0.90980392,\n",
       "        0.99607843, 0.99607843, 0.98431373, 0.8627451 , 0.69411765,\n",
       "        0.56078431, 0.86666667, 0.91372549, 0.87058824, 0.85098039,\n",
       "        0.89803922, 0.85098039, 0.36078431, 0.03921569, 0.01176471,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.08627451,\n",
       "        0.62352941, 0.85490196, 0.99215686, 0.99607843, 0.98823529,\n",
       "        0.74117647, 0.32941176, 0.18431373, 0.07058824, 0.41960784,\n",
       "        0.36078431, 0.20392157, 0.16470588, 0.50588235, 0.59215686,\n",
       "        0.10196078, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.18039216, 0.81176471, 0.95686275,\n",
       "        0.99607843, 0.99607843, 0.98039216, 0.56470588, 0.15686275,\n",
       "        0.20392157, 0.0745098 , 0.2       , 0.1254902 , 0.04313725,\n",
       "        0.05098039, 0.45882353, 0.58039216, 0.10588235, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01568627,\n",
       "        0.45098039, 0.96078431, 0.99607843, 0.99607843, 0.91764706,\n",
       "        0.81176471, 0.30196078, 0.11764706, 0.29411765, 0.10588235,\n",
       "        0.00784314, 0.        , 0.        , 0.1254902 , 0.54901961,\n",
       "        0.39607843, 0.03137255, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01176471, 0.08627451, 0.6745098 , 0.98823529,\n",
       "        0.99607843, 0.98431373, 0.56862745, 0.25098039, 0.04313725,\n",
       "        0.0745098 , 0.08627451, 0.10196078, 0.14509804, 0.14509804,\n",
       "        0.15294118, 0.39607843, 0.83137255, 0.68235294, 0.20784314,\n",
       "        0.14509804, 0.1254902 , 0.02745098, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03137255,\n",
       "        0.18039216, 0.81568627, 0.99607843, 0.99607843, 0.98039216,\n",
       "        0.50588235, 0.16078431, 0.05490196, 0.20392157, 0.18431373,\n",
       "        0.3254902 , 0.49019608, 0.49803922, 0.50588235, 0.69411765,\n",
       "        0.94117647, 0.87058824, 0.55294118, 0.49019608, 0.44313725,\n",
       "        0.12941176, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1254902 , 0.44705882, 0.96078431,\n",
       "        0.99607843, 0.99607843, 0.98039216, 0.49803922, 0.14901961,\n",
       "        0.14901961, 0.66666667, 0.81176471, 0.91372549, 0.97647059,\n",
       "        0.98039216, 0.98039216, 0.98823529, 0.99607843, 0.99607843,\n",
       "        0.98039216, 0.97647059, 0.95294118, 0.61960784, 0.07843137,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14509804, 0.49803922, 0.98039216, 0.99607843, 1.        ,\n",
       "        0.98039216, 0.49803922, 0.16078431, 0.38823529, 0.9254902 ,\n",
       "        0.98039216, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "        0.99607843, 1.        , 0.99607843, 0.99607843, 0.99607843,\n",
       "        0.99215686, 0.78431373, 0.1254902 , 0.01176471, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14509804, 0.49803922,\n",
       "        0.98039216, 1.        , 1.        , 0.98039216, 0.49803922,\n",
       "        0.15686275, 0.29803922, 0.83529412, 0.90588235, 0.96470588,\n",
       "        0.98039216, 0.98823529, 0.99607843, 0.99607843, 0.99607843,\n",
       "        0.99607843, 0.99607843, 0.98039216, 0.95294118, 0.61960784,\n",
       "        0.07843137, 0.00784314, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.14509804, 0.49803922, 0.98039216, 1.        ,\n",
       "        1.        , 0.98039216, 0.55294118, 0.20784314, 0.04705882,\n",
       "        0.25882353, 0.32156863, 0.45098039, 0.50588235, 0.74901961,\n",
       "        0.9254902 , 0.99607843, 0.99607843, 0.99607843, 0.87058824,\n",
       "        0.54509804, 0.45098039, 0.12941176, 0.00392157, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.14509804,\n",
       "        0.49803922, 0.98039216, 0.99607843, 1.        , 0.99607843,\n",
       "        0.87058824, 0.69019608, 0.32941176, 0.14117647, 0.09019608,\n",
       "        0.03137255, 0.03921569, 0.50588235, 0.85490196, 0.99607843,\n",
       "        0.99607843, 0.98039216, 0.55294118, 0.05098039, 0.01568627,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.14509804, 0.49019608, 0.97647059,\n",
       "        0.99607843, 0.99607843, 0.99607843, 0.96862745, 0.90980392,\n",
       "        0.6745098 , 0.45098039, 0.32156863, 0.13333333, 0.05490196,\n",
       "        0.50588235, 0.85490196, 0.99607843, 0.99607843, 0.98039216,\n",
       "        0.50980392, 0.02352941, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.07843137, 0.30196078, 0.80784314, 0.96078431, 0.98039216,\n",
       "        0.99607843, 0.99607843, 0.99607843, 0.98823529, 0.96078431,\n",
       "        0.91372549, 0.8       , 0.56078431, 0.74901961, 0.9254902 ,\n",
       "        0.99607843, 0.97254902, 0.9254902 , 0.59607843, 0.0627451 ,\n",
       "        0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02745098, 0.1254902 ,\n",
       "        0.49411765, 0.8       , 0.8627451 , 0.96078431, 0.98039216,\n",
       "        0.98039216, 0.99607843, 0.99607843, 0.98823529, 0.96078431,\n",
       "        0.87058824, 0.9254902 , 0.97647059, 0.98431373, 0.87058824,\n",
       "        0.7372549 , 0.50196078, 0.05882353, 0.00392157, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.03137255, 0.13333333,\n",
       "        0.19607843, 0.44705882, 0.50588235, 0.55294118, 0.8627451 ,\n",
       "        0.97647059, 0.98039216, 0.98039216, 0.97647059, 0.91372549,\n",
       "        0.8627451 , 0.66666667, 0.30980392, 0.14901961, 0.0627451 ,\n",
       "        0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01568627,\n",
       "        0.01960784, 0.04313725, 0.30196078, 0.49019608, 0.49803922,\n",
       "        0.49803922, 0.49019608, 0.32156863, 0.19607843, 0.08235294,\n",
       "        0.01176471, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.07843137, 0.14509804, 0.14509804, 0.14509804, 0.14509804,\n",
       "        0.08235294, 0.03529412, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_image(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1397af07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "        0.01568627, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.07843137, 0.42745098,\n",
       "        0.44705882, 0.17647059, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.44705882, 0.96078431,\n",
       "        0.99215686, 0.83921569, 0.01960784, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.07843137, 0.39215686,\n",
       "        0.12941176, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.49019608, 0.97647059,\n",
       "        0.99607843, 0.91372549, 0.08627451, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.03921569, 0.48235294, 0.93333333,\n",
       "        0.63921569, 0.12941176, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00784314, 0.01568627, 0.01568627, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.32156863, 0.91372549,\n",
       "        0.99607843, 0.98431373, 0.32156863, 0.00784314, 0.        ,\n",
       "        0.        , 0.01176471, 0.30980392, 0.8627451 , 0.99607843,\n",
       "        0.95686275, 0.49411765, 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.30196078, 0.49019608, 0.44313725, 0.03137255,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 0.99607843, 0.50196078, 0.01960784, 0.        ,\n",
       "        0.        , 0.13333333, 0.8       , 0.99607843, 1.        ,\n",
       "        0.99607843, 0.84705882, 0.01960784, 0.        , 0.08627451,\n",
       "        0.60392157, 0.96862745, 0.99607843, 0.99215686, 0.43529412,\n",
       "        0.01176471, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 0.99607843, 0.62745098, 0.0627451 , 0.        ,\n",
       "        0.01176471, 0.32941176, 0.91372549, 0.99607843, 1.        ,\n",
       "        0.99607843, 0.91372549, 0.08627451, 0.03921569, 0.37254902,\n",
       "        0.90980392, 0.99607843, 0.99607843, 0.96470588, 0.30196078,\n",
       "        0.00784314, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.15294118, 0.85098039,\n",
       "        0.99607843, 0.99607843, 0.72156863, 0.09803922, 0.        ,\n",
       "        0.13333333, 0.68627451, 0.98431373, 0.99607843, 1.        ,\n",
       "        0.99607843, 0.98039216, 0.32156863, 0.50196078, 0.86666667,\n",
       "        0.99215686, 1.        , 0.98823529, 0.69411765, 0.03137255,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.45098039, 0.96078431,\n",
       "        0.99607843, 0.99607843, 0.85098039, 0.14509804, 0.03529412,\n",
       "        0.68627451, 0.98431373, 0.99607843, 1.        , 1.        ,\n",
       "        1.        , 0.99607843, 0.98431373, 0.99607843, 0.99607843,\n",
       "        0.99607843, 0.99607843, 0.90980392, 0.42745098, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.00392157, 0.61568627, 0.98431373,\n",
       "        0.99607843, 1.        , 0.85098039, 0.15686275, 0.30196078,\n",
       "        0.96470588, 0.99607843, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.99607843, 0.94901961, 0.51372549, 0.08627451, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.36862745, 0.91764706,\n",
       "        0.99607843, 1.        , 0.85490196, 0.19607843, 0.54901961,\n",
       "        0.99607843, 0.99607843, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99607843,\n",
       "        0.98823529, 0.51372549, 0.1254902 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.08235294, 0.6745098 ,\n",
       "        0.98823529, 1.        , 0.97647059, 0.89019608, 0.96862745,\n",
       "        0.99607843, 0.99215686, 0.9372549 , 0.97254902, 0.99607843,\n",
       "        1.        , 1.        , 1.        , 0.99607843, 0.99215686,\n",
       "        0.81176471, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.03921569, 0.55686275,\n",
       "        0.98431373, 1.        , 0.99607843, 0.99607843, 0.99607843,\n",
       "        0.99607843, 0.91764706, 0.45490196, 0.64705882, 0.99215686,\n",
       "        0.99607843, 1.        , 1.        , 0.99607843, 0.91764706,\n",
       "        0.37254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.1254902 , 0.79607843,\n",
       "        0.99607843, 1.        , 1.        , 1.        , 0.99607843,\n",
       "        0.99215686, 0.79215686, 0.1372549 , 0.14509804, 0.92156863,\n",
       "        0.99607843, 1.        , 1.        , 0.99607843, 0.85098039,\n",
       "        0.15294118, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 1.        , 1.        , 1.        , 0.99607843,\n",
       "        0.86666667, 0.35686275, 0.02745098, 0.08235294, 0.91764706,\n",
       "        0.99607843, 1.        , 1.        , 0.99607843, 0.84313725,\n",
       "        0.14509804, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 1.        , 1.        , 0.99607843, 0.99607843,\n",
       "        0.45098039, 0.01568627, 0.        , 0.1254902 , 0.96078431,\n",
       "        0.99607843, 1.        , 0.99607843, 0.96470588, 0.49803922,\n",
       "        0.03137255, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 1.        , 1.        , 0.99607843, 0.95686275,\n",
       "        0.18039216, 0.        , 0.        , 0.03529412, 0.87058824,\n",
       "        0.99607843, 1.        , 0.99607843, 0.81568627, 0.18039216,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 1.        , 1.        , 0.98823529, 0.69411765,\n",
       "        0.02745098, 0.        , 0.        , 0.01568627, 0.85098039,\n",
       "        0.99607843, 1.        , 0.98039216, 0.54901961, 0.03529412,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 1.        , 0.99607843, 0.96078431, 0.44705882,\n",
       "        0.        , 0.        , 0.        , 0.01568627, 0.85098039,\n",
       "        0.99607843, 1.        , 0.91372549, 0.32156863, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 1.        , 0.99607843, 0.8627451 , 0.19607843,\n",
       "        0.        , 0.        , 0.        , 0.01568627, 0.85098039,\n",
       "        0.99607843, 0.99607843, 0.85098039, 0.15294118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.14509804, 0.85098039,\n",
       "        0.99607843, 1.        , 0.96862745, 0.62352941, 0.07843137,\n",
       "        0.        , 0.        , 0.        , 0.01568627, 0.79607843,\n",
       "        0.99607843, 0.99607843, 0.79607843, 0.1254902 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.32156863, 0.91372549,\n",
       "        0.99607843, 0.99607843, 0.84705882, 0.14901961, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.45098039,\n",
       "        0.94117647, 0.91372549, 0.43137255, 0.01568627, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.49019608, 0.97647059,\n",
       "        0.99607843, 0.99607843, 0.66666667, 0.08235294, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1254902 ,\n",
       "        0.43529412, 0.32156863, 0.07058824, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.44705882, 0.96078431,\n",
       "        0.99607843, 0.98431373, 0.3254902 , 0.01176471, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.00784314, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.07843137, 0.42745098,\n",
       "        0.49803922, 0.44313725, 0.02745098, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "        0.01568627, 0.01568627, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = images/255.0\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b7cd1680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = to_categorical(labels)\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15670b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

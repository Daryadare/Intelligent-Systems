{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d495b6e",
   "metadata": {},
   "source": [
    "## <center>Лабораторная работа № 8 'Генерация текста на основе “Алисы в стране чудес”'<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd92e35",
   "metadata": {},
   "source": [
    "### <center>Выполнила студентка 3 курса группы БФИ2001 Калмыкова Дарья<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d12577",
   "metadata": {},
   "source": [
    "### Цель\n",
    "Использовать рекуррентные нейронные сети в качестве генеративных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29bdd9",
   "metadata": {},
   "source": [
    "### Задачи\n",
    "* Ознакомиться с генерацией текста\n",
    "* Ознакомиться с системой Callback в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdacbfe",
   "metadata": {},
   "source": [
    "### Требования\n",
    "1. Реализовать модель ИНС, которая будет генерировать текст\n",
    "2. Написать собственный CallBack, который будет показывать то как генерируется \n",
    "текст во время обучения (то есть раз в какое-то количество эпох генирировать и \n",
    "выводить текст у необученной модели)\n",
    "3. Отследить процесс обучения при помощи TensorFlowCallBack (TensorBoard), в \n",
    "отчете привести результаты и их анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1925dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935e949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObj = codecs.open( \"./wonderland.txt\", \"r\", \"utf_8\" )\n",
    "raw_text = fileObj.read()\n",
    "text_clear = re.sub(r\"[\\r\\n]\", '', raw_text)\n",
    "raw_text = text_clear.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2ff989",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c44789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c5a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a2cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  141208\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "print(\"Total Characters: \", n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e917340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocab:  48\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de4d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  141108\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e33235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6edac89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73759ef0",
   "metadata": {},
   "source": [
    "#### Custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afd26c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, int_to_char):\n",
    "        self.dataX = dataX\n",
    "        self.int_to_char = int_to_char\n",
    "    \n",
    "    def period_text_gen(self, size):\n",
    "        start = np.random.randint(0, n_patterns-1)\n",
    "        pattern = self.dataX[start]\n",
    "        text = []\n",
    "        for i in range(size):\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(n_vocab)\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            index = np.argmax(prediction)\n",
    "            result = self.int_to_char[index]\n",
    "            text.append(result)\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "        return \"\".join(text)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch}\\n')\n",
    "            gen_text = self.period_text_gen(100)\n",
    "            print(f'Generated text: {gen_text}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b2a34",
   "metadata": {},
   "source": [
    "#### Using ModelCheckpoint and TensorBoard callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92ab573",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d253efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "\n",
    "log_dir = \"lab8_logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "call1 = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf8dec86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.9891\n",
      "Epoch 1: loss improved from inf to 2.98907, saving model to weights-improvement-01-2.9891.hdf5\n",
      "Epoch 0\n",
      "\n",
      "Generated text:  toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe\n",
      "\n",
      "1103/1103 [==============================] - 335s 301ms/step - loss: 2.9891\n",
      "Epoch 2/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.8326\n",
      "Epoch 2: loss improved from 2.98907 to 2.83258, saving model to weights-improvement-02-2.8326.hdf5\n",
      "1103/1103 [==============================] - 336s 305ms/step - loss: 2.8326\n",
      "Epoch 3/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.7484\n",
      "Epoch 3: loss improved from 2.83258 to 2.74843, saving model to weights-improvement-03-2.7484.hdf5\n",
      "1103/1103 [==============================] - 341s 309ms/step - loss: 2.7484\n",
      "Epoch 4/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.6809\n",
      "Epoch 4: loss improved from 2.74843 to 2.68092, saving model to weights-improvement-04-2.6809.hdf5\n",
      "1103/1103 [==============================] - 341s 309ms/step - loss: 2.6809\n",
      "Epoch 5/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.6235\n",
      "Epoch 5: loss improved from 2.68092 to 2.62355, saving model to weights-improvement-05-2.6235.hdf5\n",
      "1103/1103 [==============================] - 343s 311ms/step - loss: 2.6235\n",
      "Epoch 6/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.5671\n",
      "Epoch 6: loss improved from 2.62355 to 2.56708, saving model to weights-improvement-06-2.5671.hdf5\n",
      "Epoch 5\n",
      "\n",
      "Generated text: nd she sooe to the wooee to the wooee to the wooee to the wooee to the wooee to the wooee to the woo\n",
      "\n",
      "1103/1103 [==============================] - 369s 335ms/step - loss: 2.5671\n",
      "Epoch 7/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.5147\n",
      "Epoch 7: loss improved from 2.56708 to 2.51465, saving model to weights-improvement-07-2.5147.hdf5\n",
      "1103/1103 [==============================] - 408s 369ms/step - loss: 2.5147\n",
      "Epoch 8/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.4692\n",
      "Epoch 8: loss improved from 2.51465 to 2.46919, saving model to weights-improvement-08-2.4692.hdf5\n",
      "1103/1103 [==============================] - 732s 664ms/step - loss: 2.4692\n",
      "Epoch 9/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.4269\n",
      "Epoch 9: loss improved from 2.46919 to 2.42687, saving model to weights-improvement-09-2.4269.hdf5\n",
      "1103/1103 [==============================] - 595s 539ms/step - loss: 2.4269\n",
      "Epoch 10/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.3882\n",
      "Epoch 10: loss improved from 2.42687 to 2.38822, saving model to weights-improvement-10-2.3882.hdf5\n",
      "1103/1103 [==============================] - 417s 378ms/step - loss: 2.3882\n",
      "Epoch 11/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.3504\n",
      "Epoch 11: loss improved from 2.38822 to 2.35037, saving model to weights-improvement-11-2.3504.hdf5\n",
      "Epoch 10\n",
      "\n",
      "Generated text: o whe more to the woile ”hu  a dare cn toule to toe care to the whil  she was tott al cnl th the whr\n",
      "\n",
      "1103/1103 [==============================] - 425s 385ms/step - loss: 2.3504\n",
      "Epoch 12/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.3126\n",
      "Epoch 12: loss improved from 2.35037 to 2.31257, saving model to weights-improvement-12-2.3126.hdf5\n",
      "1103/1103 [==============================] - 435s 394ms/step - loss: 2.3126\n",
      "Epoch 13/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.2802\n",
      "Epoch 13: loss improved from 2.31257 to 2.28021, saving model to weights-improvement-13-2.2802.hdf5\n",
      "1103/1103 [==============================] - 436s 395ms/step - loss: 2.2802\n",
      "Epoch 14/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.2452\n",
      "Epoch 14: loss improved from 2.28021 to 2.24519, saving model to weights-improvement-14-2.2452.hdf5\n",
      "1103/1103 [==============================] - 434s 393ms/step - loss: 2.2452\n",
      "Epoch 15/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.2141\n",
      "Epoch 15: loss improved from 2.24519 to 2.21414, saving model to weights-improvement-15-2.2141.hdf5\n",
      "1103/1103 [==============================] - 442s 401ms/step - loss: 2.2141\n",
      "Epoch 16/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.1835\n",
      "Epoch 16: loss improved from 2.21414 to 2.18346, saving model to weights-improvement-16-2.1835.hdf5\n",
      "Epoch 15\n",
      "\n",
      "Generated text: e an the was soine to the thite was so the was an anl the was so the whit  she west hel fead an the \n",
      "\n",
      "1103/1103 [==============================] - 443s 402ms/step - loss: 2.1835\n",
      "Epoch 17/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.1522\n",
      "Epoch 17: loss improved from 2.18346 to 2.15217, saving model to weights-improvement-17-2.1522.hdf5\n",
      "1103/1103 [==============================] - 412s 373ms/step - loss: 2.1522\n",
      "Epoch 18/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.1239\n",
      "Epoch 18: loss improved from 2.15217 to 2.12385, saving model to weights-improvement-18-2.1239.hdf5\n",
      "1103/1103 [==============================] - 408s 370ms/step - loss: 2.1239\n",
      "Epoch 19/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.0939\n",
      "Epoch 19: loss improved from 2.12385 to 2.09393, saving model to weights-improvement-19-2.0939.hdf5\n",
      "1103/1103 [==============================] - 408s 370ms/step - loss: 2.0939\n",
      "Epoch 20/20\n",
      "1103/1103 [==============================] - ETA: 0s - loss: 2.0676\n",
      "Epoch 20: loss improved from 2.09393 to 2.06763, saving model to weights-improvement-20-2.0676.hdf5\n",
      "1103/1103 [==============================] - 420s 381ms/step - loss: 2.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eab1903d90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=[checkpoint, call1, Custom_Callback(dataX, int_to_char)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c842802f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 10784), started 0:00:25 ago. (Use '!kill 10784' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d67445b570b24472\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d67445b570b24472\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir lab8_logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e54e61",
   "metadata": {},
   "source": [
    "#### Epoch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8569649",
   "metadata": {},
   "source": [
    "![Epoch loss](./lab8_tb/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20359c",
   "metadata": {},
   "source": [
    "#### Time Series on Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b3928",
   "metadata": {},
   "source": [
    "![Epoch loss](./lab8_tb/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33204093",
   "metadata": {},
   "source": [
    "#### Histograms of Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950ae34",
   "metadata": {},
   "source": [
    "![](./lab8_tb/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4553e",
   "metadata": {},
   "source": [
    "#### Histograms of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45b4f5",
   "metadata": {},
   "source": [
    "![Epoch loss](./lab8_tb/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc0688",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f0bb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.0676.hdf5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bac32d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 40680\n",
      "\" abbit’s voice; and alicecalled out as loud as she could, “if you do, i’ll set dinah at you!”there wa \"\n",
      "s a lang of the sabdit worh the rame, and was sotting oo the tooee “ht, and doeng the mook tu then she was soiek the whst an the rabbit  and whnt oedt the woode oadt to the thile  and was sotting oo the tooee the was to the bare and the had not the tabbit whrh the sas oo toeee the whst an anl of the sabdit  and was sotting to the toile, and thene tas no toeeen the hoose to leke that she woudd belin the was oo the saali, and the woode had been ano aor aor aoo oo the tabli, and the marter her aele a little toile to the thile  and was soinking to the thite  atd the tooed had aele deri and toie the wast oo tee shet  the would bedin to tee the harter wo toene to her her  and thene tas a lintle toiee and toine the rabbit was the pooer wite tie tas oo the thiee  and tas soink the toeee the was to the toile, and thene tas not in the toiee “hth toee of the soeeo tfe saadit  and tas sotting to the toile the whsh tie dareen the sas oo toete tas oo the thate  and said to the woile, and the garter \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\", start)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adb889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
